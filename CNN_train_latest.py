from comet_ml import start
from comet_ml.integration.pytorch import log_model
import torch
import h5py
import numpy as np
from obspy import read
import os
from torch.utils.data import Dataset, DataLoader
import matplotlib.pyplot as plt
import torch.nn as nn
import torch.nn.functional as F
import gc
import torch.optim as optim
from tqdm import tqdm
import random

####################################noiseã€dropoutã€trend#####################################
class SeismicDatasetHDF5(Dataset):
    def __init__(self, hdf5_file, enable_noise=True, enable_dropout=True, enable_trend=True, max_noise_level=0.1):
        self.hdf5_file = hdf5_file
        with h5py.File(self.hdf5_file, 'r') as hf:
            self.target_waveforms = hf['target_waveforms'][:]
            self.egfs = hf['egfs'][:]
            self.astfs = hf['astfs'][:]
        self.enable_noise = enable_noise
        self.enable_dropout = enable_dropout
        self.enable_trend = enable_trend
        self.max_noise_level = max_noise_level

    def __len__(self):
        return len(self.target_waveforms)

    def __getitem__(self, idx):
        target_waveform = self.target_waveforms[idx]
        egf = self.egfs[idx]
        astf = self.astfs[idx]

        # é»˜è®¤å€¼
        target_noise_level = 0.0
        egf_noise_level = 0.0

        # å…ˆå®šä¹‰é»˜è®¤æ— æ‰ç‚¹å’Œæ— è¶‹åŠ¿
        target_drop_region = None
        egf_drop_region = None
        target_trend_deg = 0.0
        egf_trend_deg = 0.0

        if self.enable_noise:
            target_noise_level = np.random.uniform(0, self.max_noise_level)
            target_waveform, target_drop_region, target_trend_deg = self._augment_waveform(target_waveform, target_noise_level)

            egf_noise_level = np.random.uniform(0, self.max_noise_level)
            egf, egf_drop_region, egf_trend_deg = self._augment_waveform(egf, egf_noise_level)

        return {
            'target': torch.tensor(target_waveform, dtype=torch.float32),
            'egf': torch.tensor(egf, dtype=torch.float32),
            'astf': torch.tensor(astf, dtype=torch.float32),
            'meta': {
                'target_noise_level': target_noise_level,
                'egf_noise_level': egf_noise_level,
                'target_dropout': target_drop_region,
                'egf_dropout': egf_drop_region,
                'target_trend_deg': target_trend_deg,
                'egf_trend_deg': egf_trend_deg,
            }
        }


    def _augment_waveform(self, waveform, noise_level):
        # åŠ å™ªå£°
        if self.enable_noise:
            noise = np.random.normal(0, noise_level * np.max(np.abs(waveform)), size=waveform.shape)
            waveform = waveform + noise

        # åŠ æ‰ç‚¹
        drop_region = None
        if self.enable_dropout:
            drop_start = np.random.randint(0, len(waveform) // 2)
            drop_end = drop_start + np.random.randint(0, 10)
            waveform[drop_start:drop_end] = 0
            drop_region = (drop_start, drop_end)

        # åŠ è¶‹åŠ¿ï¼Œä½¿ç”¨è§’åº¦Â±5åº¦é™åˆ¶
        trend_deg = 0.0
        if self.enable_trend:
            # éšæœºé€‰å–ä¸€ä¸ªè§’åº¦ï¼ŒèŒƒå›´-5åˆ°5åº¦
            trend_deg = np.random.uniform(-2, 2)

            # æ³¢å½¢é•¿åº¦ä½œä¸ºxè½´é•¿åº¦ï¼Œè®¡ç®—å¯¹åº”çš„æ–œç‡ï¼ˆtan(angle)ï¼‰
            slope = np.tan(np.deg2rad(trend_deg))

            # ç”Ÿæˆè¶‹åŠ¿çº¿ y = slope * xï¼Œxä»0åˆ°len(waveform)-1
            x = np.arange(len(waveform))
            trend = slope * x

            # ä¸ºäº†è®©è¶‹åŠ¿çº¿çš„æŒ¯å¹…ä¸æ³¢å½¢æŒ¯å¹…çº§åˆ«ç›¸è¿‘ï¼Œå¯ä»¥æŒ‰æ³¢å½¢æœ€å¤§ç»å¯¹å€¼å½’ä¸€åŒ–è¶‹åŠ¿å¹…åº¦
            # ä¾‹å¦‚ä»¤è¶‹åŠ¿çº¿æœ€å¤§å€¼ä¸æ³¢å½¢æœ€å¤§ç»å¯¹å€¼ç›¸å½“
            max_abs = np.max(np.abs(waveform))
            if max_abs > 0:
                # å½’ä¸€åŒ–è¶‹åŠ¿çº¿æŒ¯å¹…
                trend = trend / np.max(np.abs(trend)) * max_abs

            waveform = waveform + trend

        return waveform, drop_region, trend_deg

    
def custom_collate(batch):
    collated = {}
    for key in batch[0]:
        if key == 'meta':
            collated[key] = [b[key] for b in batch]
        else:
            collated[key] = torch.stack([b[key] for b in batch])
    return collated


def get_dataloader_from_hdf5(hdf5_file, batch_size, enable_noise=True, enable_dropout=True, enable_trend=True, max_noise_level=0.1):
    if not (0 <= max_noise_level <= 0.1):
        raise ValueError("å™ªå£°æ¯”ä¾‹å¿…é¡»åœ¨0åˆ°0.1ä¹‹é—´")

    dataset = SeismicDatasetHDF5(
        hdf5_file,
        enable_noise=enable_noise,
        enable_dropout=enable_dropout,
        enable_trend=enable_trend,
        max_noise_level=max_noise_level
    )
    return DataLoader(dataset, batch_size=batch_size, shuffle=True,
                        num_workers=16, pin_memory=True, collate_fn=custom_collate)

class SimpleCNN(nn.Module):
     def __init__(self, in_channels=2, output_length=501):
         """
         ä¸€ä¸ªç®€å•çš„ CNN ç½‘ç»œï¼Œç”¨äºåå·ç§¯å­¦ä¹ ä»»åŠ¡ã€‚

         Args:
             in_channels (int): è¾“å…¥çš„é€šé“æ•°ï¼ˆä¾‹å¦‚ï¼Œç›®æ ‡æ³¢å½¢å’ŒEGFçš„é€šé“æ•°ä¸º2ï¼‰ã€‚
             output_length (int): è¾“å‡ºåºåˆ—çš„é•¿åº¦ï¼ˆä¾‹å¦‚ï¼Œéœ‡æºæ—¶é—´å‡½æ•°çš„é•¿åº¦ï¼‰ã€‚
         """
         super(SimpleCNN, self).__init__()

         # ç‰¹å¾æå–æ¨¡å—
         self.feature_extractor = nn.Sequential(
             nn.Conv1d(in_channels, 32, kernel_size=3, padding=1),  # è¾“å‡ºé€šé“æ•° 32ï¼Œå·ç§¯æ ¸å¤§å° 3
             nn.ReLU(),
             nn.MaxPool1d(kernel_size=2),  # æ± åŒ–ï¼Œåºåˆ—é•¿åº¦å‡åŠ

             nn.Conv1d(32, 64, kernel_size=3, padding=1),  # è¾“å‡ºé€šé“æ•° 64
             nn.ReLU(),
             nn.MaxPool1d(kernel_size=2),  # å†æ¬¡æ± åŒ–ï¼Œåºåˆ—é•¿åº¦å†å‡åŠ

             nn.Conv1d(64, 128, kernel_size=3, padding=1),  # è¾“å‡ºé€šé“æ•° 128
             nn.ReLU(),
             nn.MaxPool1d(kernel_size=2)  # æœ€åä¸€æ¬¡æ± åŒ–
         )

         # å…¨è¿æ¥æ¨¡å—
         self.classifier = nn.Sequential(
             nn.Flatten(),  # å±•å¹³ç‰¹å¾
             nn.Linear(4096, 1024),  # å‡è®¾ç»è¿‡ 3 æ¬¡æ± åŒ–ååºåˆ—é•¿åº¦å‡ä¸º output_length // 8
             nn.ReLU(),
             nn.Dropout(0.5),
             nn.Linear(1024, output_length),  # è¾“å‡ºä¸ºéœ‡æºæ—¶é—´å‡½æ•°çš„é•¿åº¦
             nn.Softplus()  # ç”¨äºç¡®ä¿è¾“å‡ºéè´Ÿ
         )

     def forward(self, target_waveform, egf):
         """
         å‰å‘ä¼ æ’­å‡½æ•°ã€‚

         Args:
             target_waveform (torch.Tensor): ç›®æ ‡æ³¢å½¢ (batch_size, seq_len)ã€‚
             egf (torch.Tensor): ç»éªŒæ ¼æ—å‡½æ•° (batch_size, seq_len)ã€‚

         Returns:
             torch.Tensor: é¢„æµ‹çš„éœ‡æºæ—¶é—´å‡½æ•° (batch_size, output_length)ã€‚
         """
         # å°† target_waveform å’Œ egf æ‹¼æ¥ä¸º (batch_size, 2, seq_len)
         x = torch.stack([target_waveform, egf], dim=1)  # åœ¨é€šé“ç»´åº¦æ‹¼æ¥
         x = self.feature_extractor(x)  # ç‰¹å¾æå–
         x = self.classifier(x)  # å…¨è¿æ¥åˆ†ç±»å™¨
         return x

class EnhancedCNN(nn.Module):
    def __init__(self, in_channels=2, output_length=501):
        """
        å¢åŠ å±‚æ•°çš„ CNN ç½‘ç»œï¼Œç”¨äºåå·ç§¯å­¦ä¹ ä»»åŠ¡ã€‚

        Args:
            in_channels (int): è¾“å…¥çš„é€šé“æ•°ï¼ˆä¾‹å¦‚ï¼Œç›®æ ‡æ³¢å½¢å’ŒEGFçš„é€šé“æ•°ä¸º2ï¼‰ã€‚
            output_length (int): è¾“å‡ºåºåˆ—çš„é•¿åº¦ï¼ˆä¾‹å¦‚ï¼Œéœ‡æºæ—¶é—´å‡½æ•°çš„é•¿åº¦ï¼‰ã€‚
        """
        super(EnhancedCNN, self).__init__()

        # ç‰¹å¾æå–æ¨¡å—
        self.feature_extractor = nn.Sequential(
            nn.Conv1d(in_channels, 32, kernel_size=3, padding=1),  # è¾“å‡ºé€šé“æ•° 32ï¼Œå·ç§¯æ ¸å¤§å° 3
            nn.ReLU(),
            nn.MaxPool1d(kernel_size=2),  # æ± åŒ–ï¼Œåºåˆ—é•¿åº¦å‡åŠ

            nn.Conv1d(32, 64, kernel_size=3, padding=1),  # è¾“å‡ºé€šé“æ•° 64
            nn.ReLU(),
            nn.MaxPool1d(kernel_size=2),  # å†æ¬¡æ± åŒ–ï¼Œåºåˆ—é•¿åº¦å†å‡åŠ

            nn.Conv1d(64, 128, kernel_size=3, padding=1),  # è¾“å‡ºé€šé“æ•° 128
            nn.ReLU(),
            nn.MaxPool1d(kernel_size=2),  # æœ€åä¸€æ¬¡æ± åŒ–

            nn.Conv1d(128, 256, kernel_size=3, padding=1),  # å¢åŠ çš„å·ç§¯å±‚
            nn.ReLU(),
            nn.MaxPool1d(kernel_size=2),  # å¢åŠ çš„æ± åŒ–å±‚ï¼Œç»§ç»­å‡å°åºåˆ—é•¿åº¦

            nn.Conv1d(256, 512, kernel_size=3, padding=1),  # å¢åŠ çš„å·ç§¯å±‚
            nn.ReLU(),
            nn.MaxPool1d(kernel_size=2)  # å†æ¬¡æ± åŒ–
        )

        # å…¨è¿æ¥æ¨¡å—
        self.classifier = nn.Sequential(
            nn.Flatten(),  # å±•å¹³ç‰¹å¾
            nn.Linear(4096, 1024),  # å¢åŠ çš„å…¨è¿æ¥å±‚
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(1024, output_length),  # è¾“å‡ºä¸ºéœ‡æºæ—¶é—´å‡½æ•°çš„é•¿åº¦
            nn.Softplus()  # ç”¨äºç¡®ä¿è¾“å‡ºéè´Ÿ
            # nn.ReLU(inplace=True)
        )

    def forward(self, target_waveform, egf):
        """
        å‰å‘ä¼ æ’­å‡½æ•°ã€‚

        Args:
            target_waveform (torch.Tensor): ç›®æ ‡æ³¢å½¢ (batch_size, seq_len)ã€‚
            egf (torch.Tensor): ç»éªŒæ ¼æ—å‡½æ•° (batch_size, seq_len)ã€‚

        Returns:
            torch.Tensor: é¢„æµ‹çš„éœ‡æºæ—¶é—´å‡½æ•° (batch_size, output_length)ã€‚
        """
        # å°† target_waveform å’Œ egf æ‹¼æ¥ä¸º (batch_size, 2, seq_len)
        x = torch.stack([target_waveform, egf], dim=1)  # åœ¨é€šé“ç»´åº¦æ‹¼æ¥
        x = self.feature_extractor(x)  # ç‰¹å¾æå–
        x = self.classifier(x)  # å…¨è¿æ¥åˆ†ç±»å™¨
        return x

class EnhancedCNN_Modified(nn.Module):
    def __init__(self, in_channels=2, output_length=501):
        """
        æ”¹è¿›ç‰ˆ EnhancedCNNï¼šé˜²æ­¢å‰ç§»+è¡¥é›¶å¸¦æ¥çš„ä¿¡æ¯ä¸¢å¤±å’Œè¿‡æ‹Ÿåˆã€‚
        """
        super(EnhancedCNN_Modified, self).__init__()

        self.feature_extractor = nn.Sequential(
            nn.Conv1d(in_channels, 32, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool1d(kernel_size=2),  # 256 â†’ 128

            nn.Conv1d(32, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool1d(kernel_size=2),  # 128 â†’ 64

            nn.Conv1d(64, 128, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool1d(kernel_size=2),  # 64 â†’ 32

            nn.Conv1d(128, 256, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool1d(kernel_size=2)  # 32 â†’ 16
            # æ³¨æ„ï¼šæ²¡æœ‰ç¬¬5å±‚æ± åŒ–ï¼Œä¿ç•™æ›´å¤šæœ‰æ•ˆä¿¡æ¯
        )

        # ä½¿ç”¨AdaptiveAvgPoolè¿›è¡Œå…¨å±€ç‰¹å¾å‹ç¼©ï¼Œé˜²æ­¢flattenç¨€ç–é—®é¢˜
        self.global_pool = nn.AdaptiveAvgPool1d(1)  # è¾“å‡ºç»´åº¦: (batch, channels, 1)

        self.classifier = nn.Sequential(
            nn.Flatten(),  # (batch, 256)
            nn.Linear(256, 512),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(512, output_length),
            nn.Softplus()  # ç”¨äºç¡®ä¿è¾“å‡ºéè´Ÿ
            # ä¸ä½¿ç”¨Softplusï¼Œlossä¸­mask padding
        )

    def forward(self, target_waveform, egf):
        """
        Args:
            target_waveform: (B, L)
            egf: (B, L)

        Returns:
            astf_pred: (B, output_length)
        """
        x = torch.stack([target_waveform, egf], dim=1)  # (B, 2, L)
        x = self.feature_extractor(x)                   # (B, 256, 16)
        x = self.global_pool(x)                         # (B, 256, 1)
        x = self.classifier(x)                          # (B, output_length)
        return x


class UNet1D(nn.Module):
    def __init__(self):
        super(UNet1D, self).__init__()
        
        # Down: ç¼–ç å™¨
        self.down1 = self.conv_block(2, 32)     # (B, 32, 256)
        self.pool1 = nn.MaxPool1d(2)            # â†’ (B, 32, 128)

        self.down2 = self.conv_block(32, 64)    # (B, 64, 128)
        self.pool2 = nn.MaxPool1d(2)            # â†’ (B, 64, 64)

        self.down3 = self.conv_block(64, 128)   # (B, 128, 64)
        self.pool3 = nn.MaxPool1d(2)            # â†’ (B, 128, 32)

        # Bottom
        self.bottom = self.conv_block(128, 256)  # (B, 256, 32)

        # Up: è§£ç å™¨
        self.up3 = self.up_block(256, 128)       # upsample to (B, 128, 64)
        self.dec3 = self.conv_block(256, 128)    # concat with down3 â†’ (B, 128, 64)

        self.up2 = self.up_block(128, 64)
        self.dec2 = self.conv_block(128, 64)

        self.up1 = self.up_block(64, 32)
        self.dec1 = self.conv_block(64, 32)

        # Final output layer
        self.final_conv = nn.Conv1d(32, 1, kernel_size=1)

    def conv_block(self, in_channels, out_channels):
        return nn.Sequential(
            nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1),
            nn.BatchNorm1d(out_channels),
            nn.ReLU(inplace=True),
            nn.Conv1d(out_channels, out_channels, kernel_size=3, padding=1),
            nn.BatchNorm1d(out_channels),
            nn.ReLU(inplace=True),
        )

    def up_block(self, in_channels, out_channels):
        return nn.Sequential(
            nn.Upsample(scale_factor=2, mode='linear', align_corners=True),
            nn.Conv1d(in_channels, out_channels, kernel_size=1)
        )

    def forward(self, x1, x2):
        x = torch.stack([x1, x2], dim=1)  # (B, 2, 256)

        # Down path
        d1 = self.down1(x)         # (B, 32, 256)
        p1 = self.pool1(d1)        # (B, 32, 128)

        d2 = self.down2(p1)        # (B, 64, 128)
        p2 = self.pool2(d2)        # (B, 64, 64)

        d3 = self.down3(p2)        # (B, 128, 64)
        p3 = self.pool3(d3)        # (B, 128, 32)

        # Bottom
        bn = self.bottom(p3)       # (B, 256, 32)

        # Up path
        up3 = self.up3(bn)         # (B, 128, 64)
        up3 = torch.cat([up3, d3], dim=1)
        dec3 = self.dec3(up3)      # (B, 128, 64)

        up2 = self.up2(dec3)       # (B, 64, 128)
        up2 = torch.cat([up2, d2], dim=1)
        dec2 = self.dec2(up2)      # (B, 64, 128)

        up1 = self.up1(dec2)       # (B, 32, 256)
        up1 = torch.cat([up1, d1], dim=1)
        dec1 = self.dec1(up1)      # (B, 32, 256)

        out = self.final_conv(dec1)  # (B, 1, 256)
        return out.squeeze(1)        # (B, 256)

####################################Loss Functions####################################
class AmplitudeWeightedMSELoss(nn.Module):
    def __init__(self, epsilon, a):
        super(AmplitudeWeightedMSELoss, self).__init__()
        self.epsilon = epsilon
        self.a = a

    def forward(self, y_pred, y_true):
        # è®¡ç®—æ¯ç»„æ•°æ®çš„æŒ¯å¹…èŒƒå›´
        amplitude = torch.max(torch.abs(y_true), dim=1, keepdim=True)[0] + self.epsilon
        
        # ä½¿ç”¨æŒ¯å¹…èŒƒå›´å½’ä¸€åŒ–
        normalized_error = (y_pred - y_true) / amplitude**self.a
        # è®¡ç®—åŠ æƒ MSE æŸå¤±
        return torch.mean(normalized_error ** 2)

class RMSELoss(nn.Module):
    def __init__(self, eps=1e-6):  # é¿å…é™¤ä»¥0
        super(RMSELoss, self).__init__()
        self.mse = nn.MSELoss()
        self.eps = eps

    def forward(self, y_pred, y_true):
        return torch.sqrt(self.mse(y_pred, y_true) + self.eps)

class EffectiveMSELoss(nn.Module):
    def __init__(self, threshold=1e-3):
        """
        å»æ‰æŒ¯å¹…åŠ æƒçš„æœ‰æ•ˆæ®µ MSE æŸå¤±å‡½æ•°ã€‚
        åªåœ¨é¢„æµ‹æˆ–æ ‡ç­¾ä¸­éé›¶çš„ä¸»ä¿¡å·æ®µï¼ˆ[start:end]ï¼‰è®¡ç®— MSEã€‚

        å‚æ•°:
        - threshold: ç›¸å¯¹æœ€å¤§å€¼çš„éé›¶åˆ¤æ–­é˜ˆå€¼ï¼ˆå»ºè®® 1e-3ï¼‰
        """
        super().__init__()
        self.threshold = threshold

    def forward(self, y_pred, y_true):
        B, L = y_pred.shape

        # æœ€å¤§å¹…å€¼ï¼ˆé˜²æ­¢é™¤ä»¥0ï¼‰
        max_p = torch.amax(torch.abs(y_pred), dim=1, keepdim=True) + 1e-6
        max_t = torch.amax(torch.abs(y_true), dim=1, keepdim=True) + 1e-6

        # éé›¶æ©ç ï¼ˆåªè¦é¢„æµ‹æˆ–æ ‡ç­¾å¤§äºé˜ˆå€¼ï¼‰
        mask_p = torch.abs(y_pred) > (self.threshold * max_p)
        mask_t = torch.abs(y_true) > (self.threshold * max_t)
        mask = mask_p | mask_t  # åˆå¹¶ï¼šé¢„æµ‹æˆ–æ ‡ç­¾æœ‰ä¿¡å·éƒ½ç®—æœ‰æ•ˆ

        # æ‰¾æœ‰æ•ˆåŒºé—´ [start, end]
        idx_start = mask.float().argmax(dim=1)  # æ¯è¡Œç¬¬ä¸€ä¸ªTrue
        idx_end = L - mask.flip(dims=[1]).float().argmax(dim=1) - 1  # æœ€åä¸€ä¸ªTrue

        # æ„é€ æ©ç  [start, end]
        range_idx = torch.arange(L, device=y_pred.device)[None, :]  # (1, L)
        mask_eff = (range_idx >= idx_start[:, None]) & (range_idx <= idx_end[:, None])  # (B, L)

        # è¯¯å·®å¹³æ–¹
        diff_sq = (y_pred - y_true) ** 2  # (B, L)
        masked_error = diff_sq * mask_eff  # åªä¿ç•™æœ‰æ•ˆæ®µè¯¯å·®

        # å¹³å‡æ¯ä¸ªæ ·æœ¬æœ‰æ•ˆæ®µçš„ MSE
        valid_counts = mask_eff.sum(dim=1).clamp(min=1)  # (B,)
        loss_per_sample = masked_error.sum(dim=1) / valid_counts

        return loss_per_sample.mean()

class EffectiveAmplitudeWeightedMSELoss(nn.Module):
    def __init__(self, epsilon=1e-6, a=0.8, threshold=1e-3):
        """
        ä½¿ç”¨ mask = mask_true | mask_pred æ¥æ„é€ æœ‰æ•ˆåŒºé—´æ©ç ã€‚
        ä»…åœ¨é¢„æµ‹å€¼æˆ–çœŸå®å€¼ä¸ºéé›¶çš„ä½ç½®ä¹‹é—´è®¡ç®—æŒ¯å¹…åŠ æƒ MSEã€‚
        
        å‚æ•°ï¼š
        - epsilon: é˜²æ­¢é™¤é›¶
        - a: æŒ¯å¹…å½’ä¸€å¹‚æ¬¡
        - threshold: ç›¸å¯¹æœ€å¤§å€¼é˜ˆå€¼ï¼Œæ§åˆ¶éé›¶åˆ¤å®š
        """
        super().__init__()
        self.epsilon = epsilon
        self.a = a
        self.threshold = threshold

    def forward(self, y_pred, y_true):
        B, L = y_pred.shape

        # æŒ¯å¹…æœ€å¤§å€¼ï¼ˆé¢„æµ‹ & æ ‡ç­¾ï¼‰
        max_p = torch.amax(torch.abs(y_pred), dim=1, keepdim=True) + self.epsilon
        max_t = torch.amax(torch.abs(y_true), dim=1, keepdim=True) + self.epsilon

        # ç›¸å¯¹é˜ˆå€¼æ©ç ï¼ˆåˆ¤æ–­éé›¶åŒºï¼‰
        mask_p = torch.abs(y_pred) > (self.threshold * max_p)
        mask_t = torch.abs(y_true) > (self.threshold * max_t)

        # åˆå¹¶æ©ç ï¼ˆé¢„æµ‹æˆ–æ ‡ç­¾ä¸­æœ‰ä¿¡å·éƒ½è§†ä¸ºæœ‰æ•ˆï¼‰
        mask = mask_p | mask_t  # shape: (B, L)

        # æ‰¾èµ·æ­¢ç‚¹ï¼ˆéé›¶æ®µçš„èµ·ç‚¹å’Œç»ˆç‚¹ï¼‰
        idx_start = mask.float().argmax(dim=1)  # æ¯è¡Œç¬¬ä¸€ä¸ª True
        idx_end = L - mask.flip(dims=[1]).float().argmax(dim=1) - 1  # æ¯è¡Œæœ€åä¸€ä¸ª True

        # æ„é€ æ©ç  [start, end] ä¹‹é—´ä¸º True
        range_idx = torch.arange(L, device=y_pred.device)[None, :]  # shape: (1, L)
        mask_eff = (range_idx >= idx_start[:, None]) & (range_idx <= idx_end[:, None])  # (B, L)

        # ä½¿ç”¨æ ‡ç­¾çš„æœ€å¤§å¹…åº¦è¿›è¡ŒæŒ¯å¹…åŠ æƒå½’ä¸€
        amp = torch.amax(torch.abs(y_true), dim=1, keepdim=True) + self.epsilon
        weight = amp ** self.a  # shape: (B, 1)

        # å½’ä¸€åŒ–è¯¯å·®å¹³æ–¹
        norm_diff_sq = ((y_pred - y_true) / weight) ** 2  # (B, L)

        # æ©ç è¿‡æ»¤
        masked_error = norm_diff_sq * mask_eff  # (B, L)

        # å¹³å‡æ¯ä¸ªæ ·æœ¬çš„ lossï¼ˆé™¤ä»¥æœ‰æ•ˆé•¿åº¦ï¼‰
        valid_counts = mask_eff.sum(dim=1).clamp(min=1)
        loss_per_sample = masked_error.sum(dim=1) / valid_counts

        return loss_per_sample.mean()

def train_model(model, train_loader, val_loader, num_epochs, criterion, optimizer, device, loss_file, pth_file, scheduler=None, patience=20, device_ids=[1, 2]):
    # print(f"Training on device: {device}")
    # model = nn.DataParallel(model, device_ids=[1])  # ä½¿ç”¨ç¬¬äºŒä¸ªGPU
    # model.to(device)
    print(f"ğŸŸ¢ Training on device(s): {device_ids}")
    
    # å¦‚æœå°šæœªå°è£…ï¼Œåˆ™å°è£… DataParallel
    if not isinstance(model, nn.DataParallel):
        model = nn.DataParallel(model, device_ids=device_ids)

    model.to(device)

    best_val_loss = float("inf")
    best_epoch = 0
    early_stop_counter = 0
    train_losses = []
    val_losses = []

    for epoch in range(num_epochs):
        model.train()
        train_loss = 0.0
        for batch in tqdm(train_loader, desc=f"Epoch {epoch + 1}/{num_epochs} - Training"):
            # ä» batch ä¸­å–å‡ºè¾“å…¥æ•°æ®
            target_waveform = batch['target'].to(device)
            egf = batch['egf'].to(device)
            astf = batch['astf'].to(device)

            optimizer.zero_grad()
            predicted_astf = model(target_waveform, egf)
            loss = criterion(predicted_astf, astf)
            # loss = criterion(predicted_astf, astf.unsqueeze(1))  # [B, 501] -> [B, 1, 501]
            train_loss += loss.item()
            loss.backward()

            if epoch == 0:
                print("\næ¢¯åº¦æ£€æŸ¥:")
                for name, param in model.named_parameters():
                    if param.grad is not None:
                        print(f"{name:30} grad mean: {param.grad.mean().item():.4f} (shape: {tuple(param.grad.shape)})")
                    else:
                        print(f"{name:30} has no gradient")

            optimizer.step()

        train_loss /= len(train_loader)
        train_losses.append(train_loss)
        print(f"Epoch {epoch + 1} - Training Loss: {train_loss:.4f}")

        # éªŒè¯
        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for batch in tqdm(val_loader, desc=f"Epoch {epoch + 1}/{num_epochs} - Validation"):
                target_waveform = batch['target'].to(device)
                egf = batch['egf'].to(device)
                astf = batch['astf'].to(device)

                predicted_astf = model(target_waveform, egf)
                loss = criterion(predicted_astf, astf)
                # loss = criterion(predicted_astf, astf.unsqueeze(1))  # [B, 501] -> [B, 1, 501]
                val_loss += loss.item()

        val_loss /= len(val_loader)
        val_losses.append(val_loss)
        print(f"Epoch {epoch + 1} - Validation Loss: {val_loss:.4f}")

        torch.cuda.empty_cache()
        gc.collect()

        if scheduler:
            scheduler.step(val_loss)

        current_lr = optimizer.param_groups[0]['lr']
        print(f"Epoch {epoch + 1} - Current Learning Rate: {current_lr:.6f}")

        save_losses(train_losses, val_losses, loss_file)

        if val_loss < best_val_loss:
            best_val_loss = val_loss
            best_epoch = epoch + 1
            early_stop_counter = 0
            torch.save(model.state_dict(), pth_file)
            print(f"Model saved with Validation Loss: {best_val_loss:.4f}")
        else:
            early_stop_counter += 1
            print(f"EarlyStopping counter: {early_stop_counter} out of {patience}")

        if early_stop_counter >= patience:
            print("Early stopping triggered. Stopping training.")
            break

    print(f"Training completed. Best model achieved at epoch {best_epoch} with Validation Loss: {best_val_loss:.4f}")
    return train_losses, val_losses


def save_losses(train_losses, val_losses, filename):
    with open(filename, 'w') as f:
        f.write("Epoch, Train Loss, Validation Loss\n")  # å†™å…¥è¡¨å¤´
        for epoch, (train_loss, val_loss) in enumerate(zip(train_losses, val_losses)):
            f.write(f"{epoch+1}, {train_loss:.4f}, {val_loss:.4f}\n")  # æ ¼å¼åŒ–å¹¶å†™å…¥æ¯ä¸€è¡Œ


################åŠ è½½æ•°æ®ï¼Œå®šä¹‰è¶…å‚æ•°#################
# train_h5 = "/group/data/dataset_ASTF-net/Target_waveforms/Train_pairs.h5"
# valid_h5 = "/group/data/dataset_ASTF-net/Target_waveforms/Validation_pairs.h5"
train_h5 = "/group/data/dataset_ASTF-net/Target_waveforms/Train_pairs_3_Samezero_501_new_3_1.h5"
valid_h5 = "/group/data/dataset_ASTF-net/Target_waveforms/Validation_pairs_3_Samezero_501_new_3_1.h5"

loss_file = "/group/prc/data/dataset_ASTF-net/loss_file/best_seismic_CNN5_3_Samezero_501_new_3_cuda1_noise_dropout_EffectiveAmpWeiMSE_1_losses.txt"
pth_file = "/group/prc/data/dataset_ASTF-net/pth_file/best_seismic_CNN5_3_Samezero_501_new_3_cuda1_noise_dropout_EffectiveAmpWeiMSE_1.pth"

batch_size = 1024 * 4

# åŠ è½½HDF5æ•°æ®å¹¶åˆ›å»ºDataLoader
train_loader = get_dataloader_from_hdf5(train_h5, batch_size, enable_noise=True, enable_dropout=True, enable_trend=False, max_noise_level=0.1)
valid_loader = get_dataloader_from_hdf5(valid_h5, batch_size, enable_noise=False, enable_dropout=False, enable_trend=False, max_noise_level=0.1)

# å‚æ•°å’Œè¶…å‚æ•°è®¾ç½®
# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# device = torch.device("cuda:1,2")  # é€‰æ‹©ç©ºé—²çš„ GPU
device = torch.device("cuda:1")  # device_ids[0]
device_ids = [1]
learning_rate = 1e-3
num_epochs = 700
# weight_decay = 1e-5

# ä½¿ç”¨ä¹‹å‰å®Œæˆçš„æ•°æ®åŠ è½½

# åˆå§‹åŒ–æ¨¡å‹
# model = SimpleCNN(in_channels=2, output_length=501)# åˆå§‹åŒ–æ¨¡å‹
# model = UNet1D()

model = EnhancedCNN(in_channels=2, output_length=501)# åˆå§‹åŒ–æ¨¡å‹
# model = AlexNet1D(in_channels=2, output_length=301)# åˆå§‹åŒ–æ¨¡å‹

# criterion = AmplitudeWeightedMSELoss(epsilon=1e-6, a=0.8)  # ä½¿ç”¨è‡ªå®šä¹‰çš„ç›¸å¯¹å‡æ–¹è¯¯å·®æŸå¤±
criterion = EffectiveAmplitudeWeightedMSELoss(a=0.8, threshold=1e-3)
# criterion = EffectiveMSELoss(threshold=1e-3)
# criterion = RMSELoss()  # ä½¿ç”¨è‡ªå®šä¹‰çš„å‡æ–¹æ ¹è¯¯å·®æŸå¤±
optimizer = optim.Adam(model.parameters(), lr=learning_rate)
scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, verbose=True)

# è®­ç»ƒæ¨¡å‹
train_model(model, train_loader, valid_loader, num_epochs, criterion, optimizer, device, loss_file, pth_file, scheduler, patience=50, device_ids=device_ids)
                                  
